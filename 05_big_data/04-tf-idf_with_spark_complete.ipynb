{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying in a corpus of documents (of the web, for that matter?)\n",
    "\n",
    "### The purpose of this class is to understand how a corpus of documents can be indexed in a 'simple' way. After that, we can retrieve the most relevant documents for a given query, or we could also try to perform some clustering.\n",
    "\n",
    "### To this end, we are going to use a set of sonnets by Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\r\n",
      "\r\n",
      "THE SONNETS\r\n",
      "\r\n",
      "by William Shakespeare\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                     1\r\n",
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as the riper should by time decease,\r\n",
      "  His tender heir might bear his memory:\r\n",
      "  But thou contracted to thine own bright eyes,\r\n",
      "  Feed'st thy light's flame with self-substantial fuel,\r\n",
      "  Making a famine where abundance lies,\r\n",
      "  Thy self thy foe, to thy sweet self too cruel:\r\n",
      "  Thou that art now the world's fresh ornament,\r\n",
      "  And only herald to the gaudy spring,\r\n",
      "  Within thine own bud buriest thy content,\r\n",
      "  And tender churl mak'st waste in niggarding:\r\n",
      "    Pity the world, or else this glutton be,\r\n",
      "    To eat the world's due, by the grave and thee.\r\n",
      "\r\n",
      "\r\n",
      "                     2\r\n",
      "  When forty winters shall besiege thy brow,\r\n",
      "  And dig deep trenches in thy beauty's field,\r\n",
      "  Thy youth's proud livery so gazed on now,\r\n",
      "  Will be a tattered weed of small worth held:\r\n",
      "  Then being asked, where all thy beauty lies,\r\n",
      "  Where all the treasure of thy lusty days;\r\n",
      "  To say within thine own deep sunken eyes,\r\n",
      "  Were an all-eating shame, and thriftless praise.\r\n",
      "  How much more praise deserved thy beauty's use,\r\n",
      "  If thou couldst answer 'This fair child of mine\r\n",
      "  Shall sum my count, and make my old excuse'\r\n",
      "  Proving his beauty by succession thine.\r\n",
      "    This were to be new made when thou art old,\r\n",
      "    And see thy blood warm when thou feel'st it cold.\r\n"
     ]
    }
   ],
   "source": [
    "# path to the file. You should point this to your data\n",
    "file = '../data/shakespeare.txt'\n",
    "\n",
    "# see the document, using the 'head' command\n",
    "!head -n40 $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that if we simply invoke the method `sc.textFile` on the input file, we will read the data line by line. For this exercise we want to create a corpus of documents; hence, we will read the data paragraph by paragraph, so that each paragraph will be like a document.\n",
    "\n",
    "### The method `newAPIHadoopFile` is quite advanced, so don't worry if you are not familiar with it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myTextFileOpening(file_, sc_, delimiter=' '):\n",
    "    return sc_.newAPIHadoopFile(file_, \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    \"org.apache.hadoop.io.LongWritable\", \"org.apache.hadoop.io.Text\",\n",
    "    conf={\"textinputformat.record.delimiter\": delimiter}).map(lambda l:l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609',\n",
       " '',\n",
       " 'THE SONNETS',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '                     1',\n",
       " '  From fairest creatures we desire increase,']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTextFileOpening(file, sc, delimiter='\\n').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609',\n",
       " 'THE SONNETS',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " \"                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data by paragraphs\n",
    "paragraphs = myTextFileOpening(file, sc, delimiter='\\n\\n')\n",
    "\n",
    "paragraphs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One we have our corpus of documents, we have to perform some cleaning on it. For this, we will be using `flatMap` operation form Spark:\n",
    "\n",
    "![flatMap](https://image.slidesharecdn.com/apachespark-141115145614-conversion-gate01/95/apache-spark-with-scala-28-638.jpg?cb=1416065824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ey cmo te pasas to 98']\n"
     ]
    }
   ],
   "source": [
    "# clean the data: we want to remove spaces (larger than one), empty lines and punctuation\n",
    "def cleanParagraph(p):\n",
    "    # convert to lower case\n",
    "    p = p.lower()\n",
    "    # remove head and tail spaces, if any\n",
    "    p = p.strip()\n",
    "    # remove puntuaction (everything but letters and numbers)\n",
    "    p = re.sub('[^a-z0-9 ]', '', p)\n",
    "    # replace multiple spaces by single space\n",
    "    p = re.sub(' +', ' ', p)\n",
    "    # remove empty lines from the RDD: for this, we return a list. If the list is empty\n",
    "    # the flatMap operation will remove it.\n",
    "    if p!='':\n",
    "        return [p]\n",
    "    else:\n",
    "        return []\n",
    "test= \" Ey cómo te pasas,             tío! 98\"\n",
    "print(cleanParagraph(test))\n",
    "# output.\n",
    "# \"ey cmo te pasas to 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the cleanParagraph functoin to paragraphs using flatMap\n",
    "cleanParagraphs = paragraphs.flatMap(cleanParagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609',\n",
       " 'the sonnets',\n",
       " 'by william shakespeare',\n",
       " '1 from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self thy foe to thy sweet self too cruel thou that art now the worlds fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and tender churl makst waste in niggarding pity the world or else this glutton be to eat the worlds due by the grave and thee',\n",
       " '2 when forty winters shall besiege thy brow and dig deep trenches in thy beautys field thy youths proud livery so gazed on now will be a tattered weed of small worth held then being asked where all thy beauty lies where all the treasure of thy lusty days to say within thine own deep sunken eyes were an alleating shame and thriftless praise how much more praise deserved thy beautys use if thou couldst answer this fair child of mine shall sum my count and make my old excuse proving his beauty by succession thine this were to be new made when thou art old and see thy blood warm when thou feelst it cold']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize some of the 'documents' to double check that everything went well\n",
    "cleanParagraphs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the purpose of querying in the corpus, it's convinient to index the documents with `zipWithIndex`. See the differece with `zipWithUniqueId`.\n",
    "\n",
    "### Also, we are going to persist the data in main memory, since we may want to perform several queries. The  level of persistance can be changed usgin the `StorageLevel`, for instance `persist(StorageLevel.MEMORY_AND_DISK)`. In order to see the level of persistance you can use the method `getStorageLevel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add index to 'documents'. \n",
    "cleanParagraphsWithIndex = (cleanParagraphs\n",
    "                            .zipWithIndex() \n",
    "                            .map(lambda x: (x[1],x[0])))\n",
    "# persist the data in RAM \n",
    "cleanParagraphsWithIndex.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '1609'), (1, 'the sonnets'), (2, 'by william shakespeare')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanParagraphsWithIndex.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "print(cleanParagraphsWithIndex.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "cleanParagraphsWithIndex.unpersist()\n",
    "cleanParagraphsWithIndex.persist(StorageLevel.MEMORY_ONLY_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "cleanParagraphsWithIndex.take(3)\n",
    "print(cleanParagraphsWithIndex.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(cleanParagraphsWithIndex.getNumPartitions())\n",
    "cleanParagraphsWithIndex = cleanParagraphsWithIndex.repartition(10).cache()\n",
    "print(cleanParagraphsWithIndex.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "print(cleanParagraphsWithIndex.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6376 \"documents\"\n"
     ]
    }
   ],
   "source": [
    "# number of documents (that is, paragraphs) in our corpus:\n",
    "print('There are %d \"documents\"' % cleanParagraphsWithIndex.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample() missing 2 required positional arguments: 'withReplacement' and 'fraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f89339af3d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleanParagraphsWithIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sample() missing 2 required positional arguments: 'withReplacement' and 'fraction'"
     ]
    }
   ],
   "source": [
    "cleanParagraphsWithIndex.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our next task is to *label* each document. One simple approach is to use the well-known  *term frequency–inverse document frequency*, or TF-IDF. Recall what we have seen in the class, and have a look at wikipedia!!!\n",
    "\n",
    "### In the following, we will be calculating separatedly the TF and IDF terms in a scalable way. Then, we will combine them by *collecting* the IDF RDD (i.e., bringing the data to memory). As I've said, this solution works here, but might fail in a larger corpus..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF: term frequency in each document\n",
    "\n",
    "### Basically, we want a histogram of words for each document in the corpus. In order to leverage the importance of larger documents, we use log-scale instead of the raw frequency of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(1 + n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate document frequency:\n",
    "from math import log\n",
    "\n",
    "def simpleHist(lista):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param lista: sequence of words\n",
    "    :return a historgram of the input lista\n",
    "    \"\"\"\n",
    "    dicc = {}\n",
    "    for w in lista:\n",
    "        if w in dicc:\n",
    "            dicc[w] +=1\n",
    "        else:\n",
    "            dicc[w] = 1\n",
    "    return dicc\n",
    "print(simpleHist('1 1 1 23 23 hola hola hola 1 a'.split(\" \")))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hist_for_document(doc):\n",
    "    '''\n",
    "    This method accepts a string (doc), and returns its histogram of words \n",
    "    '''\n",
    "    # split input string into array of words\n",
    "    listOfWords = doc.split(' ')\n",
    "    # return a dictionary of key = word and value = frequency of that word\n",
    "    # dicc = {x:listOfWords.count(x) for x in listOfWords}\n",
    "    dicc = simpleHist(listOfWords)\n",
    "    # convert to logarithmic scale: this step is optional, we can use directly the frequencies\n",
    "    dicc = {k: log(v+1) for k,v in dicc.items()}\n",
    "    return dicc\n",
    "\n",
    "print(create_hist_for_document('1 1 1 23 23 hola hola hola 1 a'))\n",
    "# we use assert to check the solution\n",
    "assert create_hist_for_document('1 1 1 23 23 hola hola hola 1 a') == \\\n",
    "    {'1': log(4+1), 'a': log(1+1), '23': log(2+1), 'hola': log(3+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanParagraphsWithIndex.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the function create_hist_for_document to each 'document' in the RDD\n",
    "tf = cleanParagraphsWithIndex.map(lambda docs: (docs[0],create_hist_for_document(docs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check everything went well\n",
    "tf.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inverse document frequency (IDF):\n",
    "\n",
    "### After having obtained the frequency of words in each document (the so-called TF), we may want to remove terms that are too common. For instance, words like *a*, *and* or *the* are very unlikely to represent a document. Recall from the classroom that such words are known as [*stop-words*](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "### The IDF index of a word is inversely proportional to the occurrance of that word in the corpus and thus, is useful for removing terms that appear frequently in our corpus. Note that we only take into account whether the word appear in a document, not how many times!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from math import log\n",
    "\n",
    "# get unique words in a document\n",
    "def word_in_doc(doc):\n",
    "    '''\n",
    "    This method return the unique words in a document. Since we want to \n",
    "    calculate the occurrance of words among documents, we return a list of \n",
    "    tuples (word,1). This structure will be useful later on in the reduce phase.\n",
    "    '''\n",
    "    list_words = doc.split(' ')\n",
    "    unique_words = set(list_words)\n",
    "    # emit a 1 for each word:\n",
    "    words_with_ones = [(w,1) for w in unique_words]\n",
    "    return words_with_ones\n",
    "\n",
    "# note that we only count 'tal' once (because the sentence passed to words_in_doc represents a single 'document')\n",
    "assert set(word_in_doc('hola que tal 123 tal')) == set([('tal', 1), ('123', 1), ('que', 1), ('hola', 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    log(1+nunmberOfDocs/float(1+w_f[1])\n",
    "\n",
    "$$\\log(1 + number_of_docs)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate inverse document frequency\n",
    "def inverse_doc_freq(rdd):\n",
    "    '''\n",
    "    This method calculates the inverse document frequency for each term in each document\n",
    "    Input: and rdd consisting of tuples of (doc_index, doc)\n",
    "    \n",
    "    '''\n",
    "    nunmberOfDocs = rdd.count()\n",
    "    \n",
    "    # get an RDD of words with the number of documents this word appears in\n",
    "    freqWordsInDocs = (rdd.flatMap(lambda docs: word_in_doc(docs[1]))\n",
    "                       .reduceByKey(add))\n",
    "    # inverse doc frequency: an RDD of tuples of the form (key=word,value=idf)\n",
    "    idf = freqWordsInDocs.map(lambda w_f: (w_f[0],log(1+nunmberOfDocs/float(1+w_f[1]))))\n",
    "    return idf\n",
    "\n",
    "# Check that everything went well (you can try other sentences, but be sure to change the assert statement accordingly!)\n",
    "test_corpus = sc.parallelize([(0,'hola que tal 123 tal'),(1,'hola mi amor amor'),(2,'hola que haces amor')])\n",
    "assert inverse_doc_freq(test_corpus).collectAsMap()  == {'haces': log(1+3.0/(1+1)), \n",
    "           'mi': log(1+3.0/(1+1)), '123': log(1+3.0/(1+1)), 'que': log(1+3.0/(1+2)), \n",
    "           'tal': log(1+3.0/(1+1)), 'amor': log(1+3.0/(1+2)), 'hola': log(1+3.0/(1+3))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF\n",
    "\n",
    "### Now, we have to combine the TF and IDF RDDs. Remember that TF is an RDD of tuples:\n",
    "###    (index_of_doc, dictionary of word frequencies)\n",
    "### and IDF is a RDD of tuples: \n",
    "###    (word, idf)\n",
    "\n",
    "### There are several approaches to combine both RDDs. For this excersise, we will follow the simplest approach: collect the IDF RDD as a dictionary. It should be clear that, for a sufficiently large corpus, bringing this data to memory can cause a memoryOutOfBounds exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this might be unsafe!!!\n",
    "idf = inverse_doc_freq(cleanParagraphsWithIndex).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Some idf: %s' % [(i,idf[i]) for i in idf][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put toghether the tf and the idf\n",
    "tfidf = tf.mapValues(lambda dicc: {word:tf_*idf[word] for word, tf_ in dicc.items()})\n",
    "tfidf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that until we don't execute this cell, nothing is actually done\n",
    "tfidf.take(4)\n",
    "# also, note that if you execute the cell again, the result appears inmediatly, because we persisted the RDD tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Advanced**: when working on distributed systems, the above will fail. We need to use broadcast variables!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_br = sc.broadcast(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_br.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.mapValues(lambda dicc: {k:idf_br.value[k]*tf for k,tf in dicc.items()}).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise:\n",
    "\n",
    "### Combine the TF and IDF RDDs with a join operation. \n",
    "\n",
    "### Remember that TF is an RDD of tuples:\n",
    "###    (index_of_doc, dictionary of word frequencies)\n",
    "### and IDF is a RDD of tuples: \n",
    "###    (word, idf)\n",
    "\n",
    "### Thus, we have to cast TF as:\n",
    "###    (word, {index_of_doc: word_frequency})\n",
    "### Perform the join with idf, so as to get:\n",
    "###    (word, (idf, {index_of_doc: word_frequency}))\n",
    "### Multiply the idf and word_frequencies:\n",
    "###    (word, {index_of_doc: idf*word_frequency})\n",
    "### And finally return to the requiered structure:\n",
    "### (index_of_doc, {word: idf*word_frequency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_rdd = inverse_doc_freq(cleanParagraphsWithIndex).cache()\n",
    "idf_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_join_tuple(t):\n",
    "    \"\"\"\n",
    "    :params t: tuple of the form (word, ( (doc_id, tf), idf))\n",
    "    \n",
    "    \"\"\"\n",
    "    word = t[0]\n",
    "    v = t[1]\n",
    "    doc_id = v[0][0]\n",
    "    tf_idf = v[0][1] * v[1]\n",
    "    return (doc_id, (word, tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flatMap(lambda x: [(w, (x[0],tf_)) for w, tf_ in x[1].items()]).join(idf_rdd).map(split_join_tuple).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(tf\n",
    " .flatMap(lambda x: [(w, (x[0],tf_)) for w, tf_ in x[1].items()])\n",
    " .join(idf_rdd)\n",
    " .map(split_join_tuple)\n",
    " .groupByKey()\n",
    " .mapValues(lambda iter_: dict(list(iter_)))\n",
    ").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying\n",
    "\n",
    "### We are finally in a position to perform our queries on the corpus. The idea is to retrieve the most relevant docuemnts for a given set of words (i.e, the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_paragraphs(tfidf =None, query = None, numberOfDocs = 10):\n",
    "    return (tfidf\n",
    "           .map(lambda x: (x[0],sum({x[1][k] if k in x[1] else -1 for k in query}) ))\n",
    "           .takeOrdered(numberOfDocs,key=lambda x: -x[1]))\n",
    "\n",
    "def return_docs(query = None, corpus = None, tfidf = None, numberOfDocs =10):\n",
    "    p = get_paragraphs(tfidf, query, numberOfDocs)\n",
    "    print('Top-%s paragraphs are %s' % (numberOfDocs,p))\n",
    "    \n",
    "    indexes = [i[0] for i in p]\n",
    "    return corpus.filter(lambda x: x[0] in indexes).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = ['william','shakespeare']\n",
    "return_docs(query=query, corpus=cleanParagraphsWithIndex,tfidf=tfidf,numberOfDocs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Check out that the word *william* appears several times in the last documents. Thus, we are actually retrieving relevant documents :) \n",
    "\n",
    "### However, the last document has more than 20 occurrancies of the word *william*. It seems reasonable that this document should have been returned in a higher position, right? Can you think of an explanation for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Word2Vec\n",
    "\n",
    "The famous `word2vec` algorithm generates a distributed representation of words. Take a look at the blog: http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "\n",
    "We could use this technique to find synonimes of a given query, and use them to retrieve the most relevant paragraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at teh help\n",
    "Word2Vec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split words in paragraphs\n",
    "inp = cleanParagraphs.map(lambda p: p.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set word embedding size\n",
    "word2vec = Word2Vec().setVectorSize(100).setNumIterations(10)\n",
    "# take a look at other properties!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = word2vec.fit(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find synonyms:\n",
    "synonyms = model.findSynonyms(\"love\",10)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "\n",
    "### - As you might have noticed, we are not considering plurals here. Can you think of a way to account for this?\n",
    "\n",
    "### - Spark's implementation of TF-IDF is not very robust (actually, it's still marked as *experimental*). The reason for this is that, in order to reduce the complexity of the problem, they use the so called [*hashing trick*](https://en.wikipedia.org/wiki/Feature_hashing). However, as explained in [Spark's documentation](http://spark.apache.org/docs/latest/ml-features.html#tf-idf), such implementation \"suffers from potential hash collisions, where different raw features (i.e. words) may become the same term after hashing\". \n",
    "\n",
    "### - Apart form querying on the corpus, one thing that could be done is to perform some clustering. For this, we could use the tf-idf index to calculate the distance between any pair of documents. Once this is done, we could build the [similarity matrix](https://en.wikipedia.org/wiki/Similarity_measure) and find clusters using, for instance, PCA, k-means, etc. \n",
    "\n",
    "### Another approach, more advanced and interesting, is to apply [topic model](https://en.wikipedia.org/wiki/Topic_model) to the corpus. The idea is to find the *latent* topics in the corpus. Every document might then belong to one or more topics (i.e., clusters). One famous algorithm for Topic Model is [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), and there are implementations of it in Python or R, as well as a [scalable version in Spark](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda) (experimental, so be careful!)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
