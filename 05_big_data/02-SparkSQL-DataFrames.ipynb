{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL and DataFrames \n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs, DataSets, and DataFrames\n",
    "\n",
    "RDDs are the original interface for Spark programming.\n",
    "\n",
    "DataFrames were introduced in 1.3\n",
    "\n",
    "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of DataFrames:\n",
    "\n",
    "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
    "\n",
    "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL and DataFrames \n",
    "\n",
    "\n",
    "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
    "\n",
    "From https://spark.apache.org/docs/2.2.0/sql-programming-guide.html:\n",
    "\n",
    "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pyspark.sql module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important classes of Spark SQL and DataFrames:\n",
    "\n",
    "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
    "\n",
    "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
    "\n",
    "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
    "\n",
    "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
    "\n",
    "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
    "\n",
    "* `pyspark.sql.types` List of data types available.\n",
    "\n",
    "* `pyspark.sql.Window` For working with window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
    "\n",
    "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc861115ba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing other options to spark session:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.config(\"someoption.key\", \"somevalue\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manager',\n",
       " 'mechanic',\n",
       " 'mechanic',\n",
       " 'manager',\n",
       " 'sales',\n",
       " 'mechanic',\n",
       " 'mechanic',\n",
       " 'mechanic',\n",
       " 'manager',\n",
       " 'mechanic',\n",
       " 'manager',\n",
       " 'manager']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "ids = range(12)\n",
    "\n",
    "positions = [random.choice(['mechanic', 'sales', 'manager']) for id_ in ids]\n",
    "\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = zip(ids, positions)\n",
    "\n",
    "df = session.createDataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='manager'),\n",
       " Row(_1=1, _2='mechanic'),\n",
       " Row(_1=2, _2='mechanic'),\n",
       " Row(_1=3, _2='manager'),\n",
       " Row(_1=4, _2='sales'),\n",
       " Row(_1=5, _2='mechanic'),\n",
       " Row(_1=6, _2='mechanic'),\n",
       " Row(_1=7, _2='mechanic'),\n",
       " Row(_1=8, _2='manager'),\n",
       " Row(_1=9, _2='mechanic'),\n",
       " Row(_1=10, _2='manager'),\n",
       " Row(_1=11, _2='manager')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='manager'),\n",
       " Row(_1=1, _2='mechanic'),\n",
       " Row(_1=2, _2='mechanic'),\n",
       " Row(_1=3, _2='manager'),\n",
       " Row(_1=4, _2='sales')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| _1|      _2|\n",
      "+---+--------+\n",
      "|  0| manager|\n",
      "|  1|mechanic|\n",
      "|  2|mechanic|\n",
      "|  3| manager|\n",
      "|  4|   sales|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  A row in L{DataFrame}.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments,\n",
      " |  the fields will be sorted by names. It is not allowed to omit\n",
      " |  a named argument to represent the value is None or missing. This should be\n",
      " |  explicitly set to None in this case.\n",
      " |  \n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(age=11, name='Alice')\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row(name, age)>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive=False)\n",
      " |      Return as an dict\n",
      " |      \n",
      " |      :param recursive: turns the nested Row as dict (default: False).\n",
      " |      \n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(self, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.n\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, position='manager'),\n",
       " Row(id=1, position='mechanic'),\n",
       " Row(id=2, position='mechanic'),\n",
       " Row(id=3, position='manager'),\n",
       " Row(id=4, position='sales'),\n",
       " Row(id=5, position='mechanic'),\n",
       " Row(id=6, position='mechanic'),\n",
       " Row(id=7, position='mechanic'),\n",
       " Row(id=8, position='manager'),\n",
       " Row(id=9, position='mechanic'),\n",
       " Row(id=10, position='manager'),\n",
       " Row(id=11, position='manager')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [Row(id=id_, position=position_) for id_, position_ in zip(ids, positions)]\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|position|\n",
      "+---+--------+\n",
      "|  0| manager|\n",
      "|  1|mechanic|\n",
      "|  2|mechanic|\n",
      "|  3| manager|\n",
      "|  4|   sales|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df  = session.createDataFrame(rows)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|position|\n",
      "+---+--------+\n",
      "|  0| manager|\n",
      "|  1|mechanic|\n",
      "|  2|mechanic|\n",
      "|  3| manager|\n",
      "|  4|   sales|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.createDataFrame(zip(ids, positions), schema=['id', 'position']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "* From RDDs\n",
    "* from Hive tables\n",
    "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " '79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " '79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " '79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " '79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = session.sparkContext.textFile('coupon150720.csv')\n",
    "\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['79062005698500',\n",
       "  '1',\n",
       "  'MAA',\n",
       "  'AUH',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '56.79',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '0526',\n",
       "  '150904',\n",
       "  'OK',\n",
       "  'IAF0'],\n",
       " ['79062005698500',\n",
       "  '2',\n",
       "  'AUH',\n",
       "  'CDG',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '84.34',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '6120',\n",
       "  '150905',\n",
       "  'OK',\n",
       "  'IAF0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_rdd = rdd.map(lambda line:line.split(\",\"))\n",
    "\n",
    "split_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+------+---+---+---+---+----+------+---+----+\n",
      "|            _1| _2| _3| _4| _5| _6|    _7| _8| _9|_10|_11| _12|   _13|_14| _15|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+---+----+------+---+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W| 56.79|USD|  1|  H|  H|0526|150904| OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W| 84.34|USD|  1|  H|  H|6120|150905| OK|IAF0|\n",
      "|79062005924069|  1|CJB|MAA| 9W| 9W|  60.0|USD|  1|  H|  H|2768|150721| OK|IAA0|\n",
      "|79065668570385|  1|DEL|DXB| 9W| 9W|160.63|USD|  2|  S|  S|0546|150804| OK|INA0|\n",
      "|79065668737021|  1|AUH|IXE| 9W| 9W|152.46|USD|  1|  V|  V|0501|150803| OK|INA0|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+---+----+------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = session.createDataFrame(split_rdd)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring and specifying schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully specifying a schema\n",
    "\n",
    "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntegerType"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "types.IntegerType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(position,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [types.StructField('id', types.IntegerType()),\n",
    "          types.StructField('position', types.StringType())]\n",
    "\n",
    "my_schema = types.StructType(fields)\n",
    "\n",
    "my_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.createDataFrame(zip(ids, positions), schema=my_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From csv files\n",
    "\n",
    "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|   _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W| 56.79|USD|  1|  H|   H|0526|150904|  OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W| 84.34|USD|  1|  H|   H|6120|150905|  OK|IAF0|\n",
      "|79062005924069|  1|CJB|MAA| 9W| 9W|  60.0|USD|  1|  H|   H|2768|150721|  OK|IAA0|\n",
      "|79065668570385|  1|DEL|DXB| 9W| 9W|160.63|USD|  2|  S|   S|0546|150804|  OK|INA0|\n",
      "|79065668737021|  1|AUH|IXE| 9W| 9W|152.46|USD|  1|  V|   V|0501|150803|  OK|INA0|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.read.csv('coupon150720.csv')\n",
    "\n",
    "df_from_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|  _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W|56.79|USD|  1|  H|   H|0526|150904|  OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W|84.34|USD|  1|  H|   H|6120|150905|  OK|IAF0|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_sql = session.sql('SELECT * FROM csv.`coupon150720.csv`')\n",
    "\n",
    "df_with_sql.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From other types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameReader.json of <pyspark.sql.readwriter.DataFrameReader object at 0x7fc848cb1c50>>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.read.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|position|\n",
      "+---+--------+\n",
      "|  0| manager|\n",
      "|  1|mechanic|\n",
      "+---+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, position='manager'), Row(id=1, position='mechanic')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and selecting\n",
    "\n",
    "Syntax inspired in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, position: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = df.filter(df['id'] > 5)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|position|\n",
      "+---+--------+\n",
      "|  6|mechanic|\n",
      "|  7|mechanic|\n",
      "|  8| manager|\n",
      "|  9|mechanic|\n",
      "| 10| manager|\n",
      "| 11| manager|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`where` is exactly synonimous with `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|position|\n",
      "+---+--------+\n",
      "|  6|mechanic|\n",
      "|  7|mechanic|\n",
      "|  8| manager|\n",
      "|  9|mechanic|\n",
      "| 10| manager|\n",
      "| 11| manager|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whered = df.where(df['id'] > 5)\n",
    "whered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Extract all employee ids which correspond to managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  3|\n",
      "|  8|\n",
      "| 10|\n",
      "| 11|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['position']=='manager')\\\n",
    "    .select(df['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  3|\n",
      "|  8|\n",
      "| 10|\n",
      "| 11|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['id'])\\\n",
    "    .where(df['position']=='manager').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  3|\n",
      "|  8|\n",
      "| 10|\n",
      "| 11|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df['position'] == 'manager'].select('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13',\n",
       " '_c14']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_csv.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|           _c0|_c1|\n",
      "+--------------+---+\n",
      "|79062005698500|  1|\n",
      "|79062005698500|  2|\n",
      "|79062005924069|  1|\n",
      "|79065668570385|  1|\n",
      "|79065668737021|  1|\n",
      "|79062006192650|  1|\n",
      "|79062006192650|  2|\n",
      "|79062005733853|  1|\n",
      "|79062005836987|  1|\n",
      "|79062005836987|  2|\n",
      "|79062005836987|  3|\n",
      "|79062005836987|  4|\n",
      "|79062005836987|  5|\n",
      "|79062005836987|  6|\n",
      "|79062005836987|  7|\n",
      "|79062005862818|  1|\n",
      "|79062005862818|  2|\n",
      "|79065668498895|  1|\n",
      "|79065668498895|  2|\n",
      "|79062005534346|  1|\n",
      "+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv.select(['_c0', '_c1']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns\n",
    "\n",
    "Dataframes are immutable, since they are built on top of RDDs, so we not assing to them. We need to create new RDDS with the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9b77d4158370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anewcolumn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "df['anewcolumn'] = df['id']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('anewcolumn', df['id'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select('id', 'position', df['id'] * 100)\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions\n",
    "\n",
    "Creating `udf`s allows us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(functions.sqrt('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_log1p = functions.udf(math.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select('id', 'position', udf_log1p('id'))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_log1p_typed = functions.udf(math.log1p, types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select('id', 'position', udf_log1p_typed('id'))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this function: what is its return type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_letters(word):\n",
    "    return word[::2]\n",
    "\n",
    "odd_letters('manager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odd_udf = functions.udf(odd_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(odd_udf('position')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "Create a 'salary' field in our df. make it 30000 for mechanics, 40000 for salespeople and 70000 for managers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary(position):\n",
    "    \n",
    "    salaries = {'mechanic': 30000,\n",
    "                'sales': 40000,\n",
    "                'manager': 70000}\n",
    "    \n",
    "    return salaries.get(position)\n",
    "\n",
    "print(salary('mechanic'))\n",
    "print(salary('boss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salary_udf = functions.udf(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(salary_udf('position')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_salaries = df.withColumn('salary', \n",
    "                                 salary_udf('position').alias('salary').cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_salaries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_salaries.stat.corr('id', 'salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_salaries.stat.cov('id', 'salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .crosstab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_udf = functions.udf(lambda : random.choice(['Madrid', 'Barcelona']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = df_with_salaries.withColumn('location', location_udf()).cache() # cache() for keeping in memory the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.crosstab('position', 'location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupby('location').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupby('location').mean('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do each aggregation as in individual step, with a number of different syntaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupby('location').agg(functions.mean('salary').alias('average'), \n",
    "                            functions.stddev('salary').alias('std'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupby('location').agg({'salary': 'mean', 'id': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections\n",
    "\n",
    "Ver much like SQL joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "data = list(zip([10,11,9,10,9],\n",
    "           [5000,10000,2000,0,1000],\n",
    "           [random.choice(['Madrid', 'Barcelona', 'Sevila']) for _ in range(5)]\n",
    "           ))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = session.createDataFrame(data, schema = ['id', 'Bonus', 'location'])\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.join(new_df, on='id', how = 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df4.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_bonuses = df4.join(new_df, on=['id', 'location'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_bonuses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(StorageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each employee's salary for their location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Calculate the mean and std of salary for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_std = df4.groupby('location').agg(functions.mean('salary').alias('mean'),\\\n",
    "                                     functions.stddev_pop('salary').alias('std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the built-in functions defined in [the `pyspark.sql` module](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupBy('location').mean('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once calculated, we annotate each employee with the values for their location and calculate their z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df4.join(mean_std, on = \"location\", how = 'left')\n",
    "\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.withColumn('z_value', (df_joined['salary'] - df_joined['mean']) / df_joined['std']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.join(mean_std, on=df4['location']==mean_std['location'], how = 'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_nulls = session.createDataFrame([[101, 'manager', 120000, None],\n",
    "                                     [None, None, 1500000, 'Haiti']])\n",
    "\n",
    "the_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df4.union(the_nulls)\n",
    "\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.dropna(subset='location').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.fillna('unknown').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.fillna('unknown', subset = 'location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL querying\n",
    "\n",
    "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.registerTempTable('df6_table')\n",
    "\n",
    "session.sql('SELECT * FROM df6_table WHERE location IS NULL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"\"\"SELECT location, \n",
    "                      avg(salary) AS avg_salary, \n",
    "                      stddev_pop(salary) AS std_salary \n",
    "               FROM df6_table \n",
    "               GROUP BY location\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def zscore(value, mean, stddev):\n",
    "    return (value-mean)/stddev\n",
    "\n",
    "zscore_udf = functions.udf(zscore)\n",
    "\n",
    "df_joined.select('id',\n",
    "                 zscore_udf('salary', 'mean', 'std').alias('z-score')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_joined.registerTempTable('df5_table')\n",
    "session.udf.register('tocoto', zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('SELECT id, tocoto(salary, mean, std) AS z_score FROM df5_table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once registered, we can perform queries as complex as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "replicate the previous exercise, but with SparkSQL instead of dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperation with Pandas\n",
    "\n",
    "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df_joined.toPandas()\n",
    "\n",
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.createDataFrame(pd_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_joined.write.csv('joined_data_frame', sep = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "1. Total ticket amounts per origin\n",
    "2. Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_coupons = session.read.csv('coupon150720.csv')\n",
    "\n",
    "raw_coupons.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = raw_coupons.select(functions.col(\"_c0\").alias(\"ticket_number\"),\n",
    "                        functions.col(\"_c1\").alias(\"cupon_number\").cast(types.IntegerType()),\n",
    "                        functions.col(\"_c2\").alias(\"origin\"),\n",
    "                        functions.col(\"_c3\").alias(\"destination\"),\n",
    "                        functions.col(\"_c4\").alias(\"airline\"),\n",
    "                        functions.col(\"_c6\").alias(\"ticket_amount\").cast(types.FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_coupons.registerTempTable('raw_coupons')\n",
    "\n",
    "coupons = session.sql(\"\"\"SELECT \n",
    "                            CAST(_c0 AS LONG) AS tkt_number, \n",
    "                            CAST(_c1 AS INT) AS cpn_number,\n",
    "                            _c2 AS origin,\n",
    "                            _c3 AS destination,\n",
    "                            _c4 AS airline,\n",
    "                            CAST(_c6 AS FLOAT) AS amount\n",
    "                        FROM raw_coupons\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['destination'] == \"MAD\"]\\\n",
    "    .groupby('origin')\\\n",
    "    .agg(functions.sum('ticket_amount')).alias('ticket_amount_per_origin')\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines = df.filter(df['destination'] == \"MAD\")\\\n",
    "    .groupby('airline')\\\n",
    "    .mean('ticket_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.sort('avg(ticket_amount)', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
    "\n",
    "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
    "\n",
    "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
