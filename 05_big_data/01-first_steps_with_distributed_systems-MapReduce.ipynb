{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps in PySpark \n",
    "\n",
    "In this notebook we will learn the fundamentals of functional programming, as well as the basic abstraction of a distributed object in Spark, the RDD. The notebook has been divided into two parts:\n",
    "\n",
    "Part 1: map/reduce basics\n",
    "\n",
    "Part 2: Work with RDD and Pair RDD abstractions \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/yogen-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: map/reduce basics\n",
    "\n",
    "![Hadoop Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Hadoop_logo.svg/220px-Hadoop_logo.svg.png)\n",
    "# **Apache Hadoop (MapReduce)**\n",
    "\n",
    "It is an open source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are common and thus should be automatically handled in software by the framework.\n",
    "\n",
    "The core of Apache Hadoop consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop splits files into large blocks and distributes them amongst the nodes in the cluster. To process the data, Hadoop MapReduce transfers packaged code for nodes to process in parallel, based on the data each node needs to process. This approach takes advantage of data locality — nodes manipulating the data that they have on hand — to allow the data to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking.\n",
    "\n",
    "![caption](http://d152j5tfobgaot.cloudfront.net/wp-content/uploads/2012/07/mapreduce.png)\n",
    "\n",
    "Since data and computation are distributed, we should avoid the use of variables, i.e. mutable data. Thus, in contrast to impertaive programming, we shall use the functional approach (lambda calculus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tuples(word): \n",
    "    return (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_reducer(tuple_1, tuple_2):\n",
    "    return (tuple_1[0], tuple_1[1] + tuple_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of the following excercises is to understand basic lambda calculus with python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Functional programming in Python\n",
    "\n",
    "So, what is Functional Programming? From Wikipedia: \n",
    "\n",
    "« …a  programing paradigm that treats computation as the evaluation of mathematical functions and **avoids changing-state and mutable  data**.»\n",
    "\n",
    "It´s based upon Lambda calculus, wich consist of:\n",
    " * Function definition (declaration of expressions)\n",
    " * Function application (evaluation of those expressions)\n",
    " * Recursion (iteration)\n",
    "\n",
    "We have already used this in python!!! :)\n",
    "\n",
    "Recall the typical \"lambda x: x+1\" we have been using as the first argument of map, reduce and filter methods:\n",
    " * **map** maps each value in the input collection to a different value. It´s just the classical mathematical funciton we are used to!\n",
    " * **reduce** takes two values from the input collection and returns a new value (of the same type) by appliying a commutative operation to them. \n",
    " * **filter** filters the elements in the input collection according to a certain (boolean) criteria.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping**\n",
    "\n",
    "![map](https://cosminpupaza.files.wordpress.com/2015/10/map.png?w=505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = range(10)\n",
    "\n",
    "list(map(lambda x: x + 10, collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x ** 2, collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'a', 'q', 't', 'e']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_of_strings = ['Hola', 'amijo', 'que', 'tal', 'estas']\n",
    "\n",
    "initials = list(map(lambda word: word[0], collection_of_strings))\n",
    "initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'a', 'q', 't', 'e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initial(word):\n",
    "    return word[0]\n",
    "\n",
    "initials = list(map(initial, collection_of_strings))\n",
    "initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 3, 3, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda word: len(word), collection_of_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering**\n",
    "![filter](https://cosminpupaza.files.wordpress.com/2015/11/filter.png?w=405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection\n",
    "\n",
    "list(filter(lambda x: x%2 ==0, collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "write a filter to extract those words that have more than 3 letters from `collection_of_strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoal', 'amijo', 'estas']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda word: len(word)>3, collection_of_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing** Recall it must be commutative! Think about the importance of this when parallelizing computations\n",
    "\n",
    "![](https://cosminpupaza.files.wordpress.com/2015/11/reduce.png?w=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "reduce(lambda x, y: x + y, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-45"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x - y, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x - y, sorted(collection, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1b) Exercise: Calculate the mean of a collection of real numbers using map/reduce\n",
    "Recall:\n",
    "\n",
    "$$\\bar x = \\frac{\\sum_{i=1}^{N} x_i}{N} $$\n",
    "\n",
    "It´s straightforward to do this with python built-in mehots sum() and len(). However, how would you do that with map/reduce? We have already shown how to sum the elements of an array. Thus, you have to calculate the length of the array. For this:\n",
    " * Create another array of the same size, consisting of 1s.\n",
    " * Sum the elements of that array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First part\n",
    "\n",
    "* Do a reduce to do the sum, and a different map-reduce to get the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ = reduce(lambda x, y: x + y, collection)\n",
    "len_ = reduce(lambda x, y: x + y, map(lambda x: 1, collection))\n",
    "sum_ / len_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine them in one pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(element):\n",
    "    return (element, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(f, collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(tuple_1, tuple_2):\n",
    "    partial_sum = tuple_1[0] + tuple_2[0]\n",
    "    partial_count = tuple_1[1] + tuple_2[1]\n",
    "    \n",
    "    return (partial_sum, partial_count)\n",
    "    \n",
    "sum_, len_ = reduce(g, map(f, collection))\n",
    "\n",
    "sum_ / len_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1c) Exercise: Calculate the standard deviation of a collection of real numbers\n",
    "Recall:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{\\sum_{i=1}^{N} (x_i-\\bar x)^2}{N-1}$$\n",
    "\n",
    "For this, use the *mean* and *count* variables from the previous excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0276503540974917"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "sum_, len_ = reduce(g, map(f, collection))\n",
    "average = sum_ / len_\n",
    "\n",
    "square_differences = map(lambda x: (x - average)**2, collection)\n",
    "sum_square_differences = reduce(lambda x, y: x + y, square_differences)\n",
    "\n",
    "math.sqrt(sum_square_differences / (len_ - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0276503540974917"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.std(collection, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in one pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_x^2 = \\frac{\\sum_{i=1}^{N} (x_i-\\bar x)^2}{N-1} =\n",
    "\\frac{\\sum_{i=1}^{N} (x_i^2+{\\bar x}^2-2x_i\\bar x)}{N-1} =\n",
    "\\frac{1}{N-1}\\left(\\sum_i x_i^2-N\\bar x^2\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1c.bis) Exercise: all at once! \n",
    "For the std calculation, we have obtained separatedly the sum of elements, the lenght and the sum of the elements squared. That is, we have swept the array three times! Can you do it in a two step process using map/reduce? Do you think it might matter at some point?\n",
    " * Hint: recall that reduce takes two arguments of the same type, and returns another value of that type. So, instead of using numbers as the elements of our array, use tuples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tuple(element):\n",
    "    return (element ** 2, element, 1)\n",
    "    \n",
    "def combine_tuples(tuple_1, tuple_2):\n",
    "    el_0 = tuple_1[0] + tuple_2[0]\n",
    "    el_1 = tuple_1[1] + tuple_2[1]\n",
    "    el_2 = tuple_1[2] + tuple_2[2]\n",
    "    \n",
    "    return (el_0, el_1, el_2)\n",
    "\n",
    "sum_squares, sum_, len_ = reduce(combine_tuples, \n",
    "                                 map(make_tuple, \n",
    "                                     collection)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0276503540974917"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt((sum_squares - len_ * (sum_ / len_) ** 2 )/ (len_ - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0276503540974917"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(collection, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1d) Twe 'word-count' problem: creating histograms\n",
    "Given a set of keys in an input collection, calculate the frequency of each key. \n",
    "\n",
    "In order to understand better how map/reduce works, we will implement this simple calculation in several forms.\n",
    "\n",
    "For simplicity, we are going to create a list of numbers between 1 and 9, that can be repeated a (random) number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "a = [random.randint(1, 9) for _ in range(500)]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d.1) Simple approach\n",
    "\n",
    " * Start with an empty dict\n",
    " * If a new key is not present in the dict, create it.\n",
    " * Otherwise, increase the frequency of the key by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 52, 2: 64, 3: 49, 4: 70, 5: 55, 6: 47, 7: 56, 8: 44, 9: 63}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(input_list):\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    for element in input_list:\n",
    "        if element not in result_dict:\n",
    "            result_dict[element] = 1\n",
    "        else:\n",
    "            result_dict[element] += 1\n",
    "            \n",
    "    return result_dict\n",
    "            \n",
    "word_count(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the dictionary before we put in the first element because:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tocoto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0147f7e17aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tocoto'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'tocoto'"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "result_dict['tocoto'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d.2) Map/reduce\n",
    "\n",
    " * Recall that *reduce* applies an operation to 2 elements of the same type, and returns another element of that type. Thus, first thing to do is to map our collection to the type of the output. We cannot use dicts, as dict(list) removes duplictaed keys. We will use list of tuples instead.\n",
    " * Then, we have to define a method in the reducer that combines keys. There are two steps:\n",
    "   * Obtain the keys in the left list\n",
    "   * Then, check that the key in the second list already exists in the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count_mr(input_list):\n",
    "    tuples = map(lambda word: [(word, 1)], input_list)\n",
    "    \n",
    "    return reduce(combine_keys, tuples)\n",
    "    \n",
    "    \n",
    "def combine_keys(left, right):\n",
    "    \n",
    "    # Copy input for purity\n",
    "    left = left.copy()\n",
    "    \n",
    "    # need to check if this is the first time we see the word\n",
    "    already_seen_words = list(map(lambda t: t[0], left))\n",
    "    this_word = right[0][0]\n",
    "    \n",
    "    if this_word not in already_seen_words:\n",
    "        left = left + right\n",
    "    else:\n",
    "        position = already_seen_words.index(this_word)\n",
    "        left[position] = (this_word, left[position][1] + 1)\n",
    "    \n",
    "    return left\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate = [('tocoto', 3), ('holi', 2)]\n",
    "tocoto = [('tocoto', 1)]\n",
    "guapi = [('guapi', 1)]\n",
    "\n",
    "assert(combine_keys(intermediate, tocoto) == [('tocoto', 4), ('holi', 2)])\n",
    "assert(combine_keys(intermediate, guapi) == [('tocoto', 3), ('holi', 2), ('guapi', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chachi': 88, 'guapi': 106, 'holi': 107, 'lerelele': 101, 'tocoto': 98}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "stupid_words = [random.choice(['holi', 'guapi', 'tocoto', 'chachi', 'lerelele']) for _ in range(500)]\n",
    "word_count(stupid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holi', 107),\n",
       " ('tocoto', 98),\n",
       " ('guapi', 106),\n",
       " ('lerelele', 101),\n",
       " ('chachi', 88)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_mr(stupid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference with the previous method, based on dictionaries: now, keys are not sorted!!\n",
    "\n",
    "But, where did we sort the keys in *word_count*??? Well, we didn´t, but python *dictionary* does that internally for us to speed up things. See the difference in time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(word_count_mr(stupid_words)) == word_count(stupid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('holi', 107), ('tocoto', 98), ('guapi', 106), ('lerelele', 101), ('chachi', 88)]\n",
      "{'holi': 107, 'tocoto': 98, 'guapi': 106, 'lerelele': 101, 'chachi': 88}\n",
      "821 µs ± 7.51 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "38.3 µs ± 531 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(word_count_mr(stupid_words))\n",
    "print(word_count(stupid_words))\n",
    "\n",
    "%timeit word_count_mr(stupid_words)\n",
    "%timeit word_count(stupid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1d.3) Map/reduce with pre-sorting**  As shown, the sorting of keys used by a dictionary actually speed up the process. \n",
    "\n",
    "However, our *combine_keys* method is creating an array of keys and checking whether a new key is already present in every step. This can be avoided by sorting the initial list first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count_mr_sorted(input_list):\n",
    "    \n",
    "    sorted_list = sorted(input_list)\n",
    "    tuples = map(lambda word: [(word, 1)], sorted_list)\n",
    "    \n",
    "    return reduce(combine_sorted_keys, tuples)\n",
    "    \n",
    "    \n",
    "def combine_sorted_keys(left, right):\n",
    "    \n",
    "    # Copy input for purity\n",
    "    left = left.copy()\n",
    "    \n",
    "    # need to check if this is the first time we see the word\n",
    "    this_word = right[0][0]\n",
    "    last_word = left[-1][0]\n",
    "    \n",
    "    already_seen = this_word == last_word\n",
    "    \n",
    "    if not already_seen:\n",
    "        left = left + right\n",
    "    else:\n",
    "        counts = left[-1][1]\n",
    "        left[-1] = (this_word, counts + 1)\n",
    "        \n",
    "    return left\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, computing times get closer. Still, our map/reduce methods are slower, since we cannot use dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('holi', 107), ('tocoto', 98), ('guapi', 106), ('lerelele', 101), ('chachi', 88)]\n",
      "{'holi': 107, 'tocoto': 98, 'guapi': 106, 'lerelele': 101, 'chachi': 88}\n",
      "[('chachi', 88), ('guapi', 106), ('holi', 107), ('lerelele', 101), ('tocoto', 98)]\n",
      "818 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "38.6 µs ± 642 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "353 µs ± 5.17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(word_count_mr(stupid_words))\n",
    "print(word_count(stupid_words))\n",
    "print(word_count_mr_sorted(stupid_words))\n",
    "\n",
    "%timeit word_count_mr(stupid_words)\n",
    "%timeit word_count(stupid_words)\n",
    "%timeit word_count_mr_sorted(stupid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Spark. Work with RDD and Pair RDD abstractions \n",
    "\n",
    "![drawing](https://prateekvjoshi.files.wordpress.com/2015/10/1-main4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ** Apache Spark**\n",
    "\n",
    "Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications.\n",
    "\n",
    "![](http://image.slidesharecdn.com/sparkandshark-120620130508-phpapp01/95/spark-and-shark-8-728.jpg?cb=1340197567)\n",
    "\n",
    "By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n",
    "![](http://spark.apache.org/images/logistic-regression.png)\n",
    "\n",
    "Spark comes with a number of components that provide flexibility and generality.\n",
    "\n",
    "<img src=\"http://spark.apache.org/images/spark-stack.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## In this part, we keep on working on the word-count example, this time with spark. The basic abstraction of Spark is the Resilient Distributed Dataset (RDD):\n",
    "\n",
    "#### «RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.»\n",
    "\n",
    " * Read only, partitioned collection of records (immutable).\n",
    " * Stores the transformations used to build a dataset (its lineage), instead of the data itself. This property ensures fault-tolerance.\n",
    " * Users can control partitioning and persistence (caching).\n",
    " * RDDs are statically typed.\n",
    " * … and yes, everything is written in scala ;p. So you better learn a little bit of it!\n",
    " \n",
    "<img src=\"http://eng.trueaccord.com/wp-content/uploads/2014/10/scala-logo.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "#### We will be trying to understand this abstraction with simple examples, using the [Python API](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2a) Create a base RDD: parallelize, actions and transformations **\n",
    "We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD.\n",
    "\n",
    "We use the sc.parallelise to convert a standard Python collection into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "stupid_words = [random.choice(['holi', \n",
    "                               'guapi', \n",
    "                               'tocoto', \n",
    "                               'chachi', \n",
    "                               'lerelele']) for _ in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD = sc.parallelize(stupid_words)\n",
    "type(wordsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-eb7382f42cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordsRDD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object does not support indexing"
     ]
    }
   ],
   "source": [
    "wordsRDD[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nothing has actually happened!**\n",
    "\n",
    "`parallellize` tells spark to distribute the data, but this is not actually done until we perform some action.\n",
    "\n",
    "Possible actions include couting, collecting, reducing, taking, etc. Take a look at the [Spark programming guide](http://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['holi', 'holi'], list)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = wordsRDD.take(2)\n",
    "res, type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from actions, we can apply transformations to an RDD. Spark won´t do anything, until an action is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pluralsRDD = wordsRDD.map(lambda word: word + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can obtain the length of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 6, 5, 5]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenghtsRDD = wordsRDD.map(lambda word: len(word))\n",
    "lenghtsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tocoto',\n",
       " 'guapi',\n",
       " 'guapi',\n",
       " 'guapi',\n",
       " 'lerelele',\n",
       " 'lerelele',\n",
       " 'chachi',\n",
       " 'guapi',\n",
       " 'guapi',\n",
       " 'lerelele']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.filter(lambda word: len(word) > 4).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurals = pluralsRDD.collect()\n",
    "\n",
    "type(plurals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2b) Persisting and the RDD lineage**\n",
    "\n",
    "So far, we have seen that Spark RDDs are *lazy evaluated*, i.e. nothing is actually done until an action is performed. In the RDD, the set of transformations to be applied are remembered: this is known as its *lineage*. It has the important consequence of making Spark RDDs *fault tolerant* automatically.\n",
    "\n",
    "![](http://images.slideplayer.com/14/4499833/slides/slide_10.jpg) \n",
    "\n",
    "It might be interesting to store some intermediate results, though: perhaps because we want to apply several different transformations starting from that point, or because we are going to apply an iterative computation (as is customary in machine learning algorithms). For this, Spark has [several ways of persisting](http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'holi'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 6]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthsRDD = wordsRDD.map(lambda word: len(word))\n",
    "lengthsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsRDD.getStorageLevel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "StorageLevel.MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'holi'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordsRDD.cache() is a synonim for:\n",
    "\n",
    "# wordsRDD.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2c) Partitioning **\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster.\n",
    "\n",
    "To get the number of partitions of an RDD, just use `getNumPartitions()` on your RDD. You can change the partitions during RDD creation (with `parallelize(collection,numPartitions)` or `fromTextFile(file,numPartitions)`), or afterwards with methos like `repartition(), coalesce()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherRDD = sc.parallelize(stupid_words, 12)\n",
    "\n",
    "otherRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitionedRDD = otherRDD.repartition(5)\n",
    "repartitionedRDD.getNumPartitions(), otherRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions using [glom()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=glom#pyspark.RDD.glom): it retruns an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "smallRDD = sc.parallelize([random.choice(['Cafe', 'Te']) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cafe', 'Cafe'],\n",
       " ['Te', 'Cafe'],\n",
       " ['Cafe', 'Cafe'],\n",
       " ['Cafe', 'Cafe', 'Te', 'Cafe'],\n",
       " ['Cafe', 'Cafe'],\n",
       " ['Cafe', 'Cafe'],\n",
       " ['Cafe', 'Cafe'],\n",
       " ['Te', 'Cafe', 'Te', 'Te']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Cafe',\n",
       "  'Te',\n",
       "  'Cafe',\n",
       "  'Te',\n",
       "  'Te'],\n",
       " ['Te', 'Cafe', 'Cafe', 'Cafe', 'Te', 'Cafe', 'Cafe', 'Cafe']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRDD.repartition(3).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitions are one of the most powerfull concepts in Spark: you can decide how to distribute your data so it can fit in memory, and more importantly, you can perform computations on each partition *before* speaking to other partitions. This can have an enorumous impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cafe', 'Cafe', 'Te', 'Cafe'],\n",
       " ['Cafe', 'Cafe', 'Cafe', 'Cafe', 'Te', 'Cafe', 'Cafe', 'Cafe'],\n",
       " ['Cafe', 'Cafe', 'Cafe', 'Cafe', 'Te', 'Cafe', 'Te', 'Te']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRDD.coalesce(3).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2c) Pair RDDs: *grouping* strategies in Spark**\n",
    "\n",
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of ('<word>', 1) for each word element in the RDD, as we did in the map/reduce version of the histogram in Python, section (1d.2).\n",
    "\n",
    "We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holi', 1),\n",
       " ('holi', 1),\n",
       " ('tocoto', 1),\n",
       " ('guapi', 1),\n",
       " ('guapi', 1),\n",
       " ('guapi', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = wordsRDD.map(lambda word: (word, 1))\n",
    "\n",
    "pairRDD.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.1) `groupByKey()` approach **\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n",
    "  + The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    " \n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterator)`. Next, sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedRDD = pairRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lerelele', <pyspark.resultiterable.ResultIterable at 0x7f8f7305ad30>),\n",
       " ('holi', <pyspark.resultiterable.ResultIterable at 0x7f8f7305ac50>),\n",
       " ('chachi', <pyspark.resultiterable.ResultIterable at 0x7f8f7305ada0>),\n",
       " ('guapi', <pyspark.resultiterable.ResultIterable at 0x7f8f7305ae10>),\n",
       " ('tocoto', <pyspark.resultiterable.ResultIterable at 0x7f8f7305ae80>)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lerelele', 101),\n",
       " ('holi', 107),\n",
       " ('chachi', 88),\n",
       " ('guapi', 106),\n",
       " ('tocoto', 98)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsRDD = groupedRDD.map(lambda tupl: (tupl[0], len(tupl[1])))\n",
    "countsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lerelele', 101),\n",
       " ('holi', 107),\n",
       " ('chachi', 88),\n",
       " ('guapi', 106),\n",
       " ('tocoto', 98)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsRDD = groupedRDD.mapValues(lambda it: len(it))\n",
    "countsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.2)  `reduceByKey` approach **\n",
    "A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. \n",
    "\n",
    "The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n",
    "\n",
    "![](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairRDD = wordsRDD.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lerelele', 101),\n",
       " ('holi', 107),\n",
       " ('chachi', 88),\n",
       " ('guapi', 106),\n",
       " ('tocoto', 98)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.reduceByKey(lambda v1, v2: v1 + v2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.3)  `combineByKey` approach: the mother of dragons **\n",
    "\n",
    "The [combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None)](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=combinebykey#pyspark.RDD.combineByKey) method is a generic (and powerful!)function to combine the elements for each key using a custom set of aggregation functions.\n",
    "\n",
    "It turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C. Note that V and C can be different – for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).\n",
    "\n",
    "Users provide three functions:\n",
    "\n",
    "#### * createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "#### * mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "#### * mergeCombiners, to combine two C’s into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's return the count and the length of words:\n",
    "wordPairs = wordsRDD....\n",
    "wordPairs.combineByKey(\n",
    "    ...,\n",
    "    ...,\n",
    "    ...\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's return the count and the list of words:\n",
    "wordPairs = wordsRDD.map(lambda i:(i,i))\n",
    "wordPairs.combineByKey(\n",
    "    ...,\n",
    "    ...,\n",
    "    ...\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (2d) Apply word count to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.1) Load a text file **\n",
    "For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, so that we only print(15 lines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-29 12:10:26--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5858792 (5,6M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   5,59M   307KB/s    in 17s     \n",
      "\n",
      "2018-03-29 12:10:44 (329 KB/s) - ‘shakespeare.txt’ saved [5858792/5858792]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -v http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r\n",
      "Project Gutenberg’s The Complete Works of William Shakespeare, by William\r",
      "\r\n",
      "Shakespeare\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r",
      "\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r",
      "\r\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms\r",
      "\r\n",
      "of the Project Gutenberg License included with this eBook or online at\r",
      "\r\n",
      "www.gutenberg.org.  If you are not located in the United States, you’ll\r",
      "\r\n",
      "have to check the laws of the country where you are located before using\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shakespeare.txt MapPartitionsRDD[38] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD = sc.textFile('shakespeare.txt')\n",
    "\n",
    "linesRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Project Gutenberg’s The Complete Works of William Shakespeare, by William',\n",
       " 'Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  You may copy it, give it away or re-use it under the terms',\n",
       " 'of the Project Gutenberg License included with this eBook or online at',\n",
       " 'www.gutenberg.org.  If you are not located in the United States, you’ll',\n",
       " 'have to check the laws of the country where you are located before using']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.2) Capitalization and punctuation **\n",
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "  + All punctuation should be removed.\n",
    "  + Any leading or trailing spaces on a line should be removed.\n",
    " \n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. Reading `help(re.sub)` might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(string):\n",
    "    \n",
    "    clean_string = re.sub('[^\\w ]', '', string)\n",
    "    \n",
    "    return clean_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'project gutenbergs the complete works of william shakespeare by william',\n",
       " 'shakespeare',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever  you may copy it give it away or reuse it under the terms',\n",
       " 'of the project gutenberg license included with this ebook or online at',\n",
       " 'wwwgutenbergorg  if you are not located in the united states youll',\n",
       " 'have to check the laws of the country where you are located before using',\n",
       " 'this ebook',\n",
       " '',\n",
       " 'see at the end of this file  content note added in 2017 ',\n",
       " '',\n",
       " '',\n",
       " 'title the complete works of william shakespeare',\n",
       " '',\n",
       " 'author william shakespeare',\n",
       " '',\n",
       " 'release date january 1994 ebook 100',\n",
       " 'last updated february 19 2018',\n",
       " '',\n",
       " 'language english',\n",
       " '',\n",
       " 'character set encoding utf8']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_linesRDD = linesRDD.map(remove_punctuation)\n",
    "\n",
    "clean_linesRDD.take(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.3) Words from lines **\n",
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the RDD:\n",
    "  + The first issue is that  that we need to split each line by its spaces.\n",
    "  + The second issue is we need to filter out empty lines.\n",
    " \n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['project',\n",
       "  'gutenbergs',\n",
       "  'the',\n",
       "  'complete',\n",
       "  'works',\n",
       "  'of',\n",
       "  'william',\n",
       "  'shakespeare',\n",
       "  'by',\n",
       "  'william'],\n",
       " ['shakespeare']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linelists_RDD = clean_linesRDD.map(lambda string: string.split(' '))\n",
    "linelists_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This could break the session, because it is going to collect \n",
    "# the whole corpus into the driver.\n",
    "\n",
    "# linelists_RDD.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_wordsRDD = clean_linesRDD\\\n",
    "                                .flatMap(lambda string: string.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.4) Remove empty elements **\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_shakespeare_wordsRDD = shakespeare_wordsRDD\\\n",
    "                            .filter(lambda word: len(word)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenbergs',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_shakespeare_wordsRDD.take(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d.5) Count the words and show the top 15\n",
    "\n",
    "We know the drill at this point, don't we? We map to a tuple then `reduceByKey`\n",
    "\n",
    "We can view the top 15 words by using the `takeOrdered()` action; however, since the elements of the RDD are pair tuples, we need a custom sort function that sorts using the value part of the pair rather than the key.\n",
    "\n",
    "You'll notice that many of the words are common English words (know as stopwords).\n",
    "\n",
    "Use our map reduce and and `takeOrdered()` to obtain the fifteen most common words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[77] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_countsRDD = only_shakespeare_wordsRDD\\\n",
    "                    .map(lambda word: (word, 1))\\\n",
    "                    .reduceByKey(lambda x, y: x + y)\n",
    "word_countsRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 90),\n",
       " ('10', 3),\n",
       " ('100', 3),\n",
       " ('1000', 1),\n",
       " ('1000txt', 1),\n",
       " ('1000zip', 1),\n",
       " ('1004', 1),\n",
       " ('1009', 1),\n",
       " ('101', 1),\n",
       " ('1012', 1),\n",
       " ('1016', 1),\n",
       " ('102', 1),\n",
       " ('1020', 1),\n",
       " ('1024', 1),\n",
       " ('1028', 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, takeOrdered sorts lexicografically, just as sorted does in raw Python\n",
    "\n",
    "word_countsRDD.takeOrdered(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 29980),\n",
       " ('and', 28353),\n",
       " ('i', 21859),\n",
       " ('to', 20811),\n",
       " ('of', 18811),\n",
       " ('a', 15984),\n",
       " ('you', 14438),\n",
       " ('my', 13191),\n",
       " ('in', 12025),\n",
       " ('that', 11782),\n",
       " ('is', 9710),\n",
       " ('not', 9068),\n",
       " ('with', 8519),\n",
       " ('me', 8270),\n",
       " ('for', 8183)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The key argument allows us to provide a custom sorting function\n",
    "\n",
    "word_countsRDD.takeOrdered(15, key=lambda t: -t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL with airline coupon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data first: coupon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coupon150720.csv  transm150720.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cps = sc.textFile('./data/coupon150720.csv').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " '79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " '79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " '79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " '79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Take fields 0, 2, 3, 4, and 6 from each line of cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coupons = cps.map(lambda line: line.split(','))\\\n",
    "             .map(lambda tupl: (tupl[0], tupl[2], tupl[3], tupl[4], float(tupl[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('79062005698500', 'MAA', 'AUH', '9W', 56.79),\n",
       " ('79062005698500', 'AUH', 'CDG', '9W', 84.34),\n",
       " ('79062005924069', 'CJB', 'MAA', '9W', 60.0),\n",
       " ('79065668570385', 'DEL', 'DXB', '9W', 160.63),\n",
       " ('79065668737021', 'AUH', 'IXE', '9W', 152.46)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Keep only the amount. Get average, max, min and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.94532037167986"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.map(lambda coupon: coupon[4]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.94532037167986"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.map(lambda coupon: coupon[4]).meanApprox(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6355194.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.map(lambda coupon: coupon[4]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.map(lambda coupon: coupon[4]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9978.4820861226926"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.map(lambda coupon: coupon[4]).stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Get stats for all tickets with destination MAD\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "1. Total ticket amounts per origin\n",
    "2. Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "madrid_coupons = coupons.filter(lambda t: t[2]=='MAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CCS', 94528.68),\n",
       " ('GRU', 87192.63999999998),\n",
       " ('EZE', 81074.63999999997),\n",
       " ('BOG', 74644.45000000001),\n",
       " ('LHR', 69609.53000000003),\n",
       " ('LPA', 60483.92),\n",
       " ('MEX', 56316.73000000001),\n",
       " ('JFK', 53496.169999999984),\n",
       " ('TLV', 53436.220000000016),\n",
       " ('TFN', 50034.949999999866)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madrid_coupons.map(lambda coupon: (coupon[1], coupon[4]))\\\n",
    "              .reduceByKey(lambda x, y: x + y)\\\n",
    "              .takeOrdered(10, lambda t: -t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('V0', (81271.48, 15)),\n",
       " ('AC', (3703.1000000000004, 5)),\n",
       " ('KE', (8950.84, 13)),\n",
       " ('SV', (25999.189999999995, 47)),\n",
       " ('OB', (4819.54, 9)),\n",
       " ('AR', (10784.14, 21)),\n",
       " ('AV', (70680.63000000008, 157)),\n",
       " ('AM', (8373.95, 19)),\n",
       " ('C2', (795.74, 2)),\n",
       " ('LA', (33815.880000000005, 89))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madrid_coupons\\\n",
    "    .map(lambda coupon: (coupon[3], (coupon[4], 1)))\\\n",
    "    .reduceByKey(lambda t1, t2: (t1[0] + t2[0], t1[1] + t2[1]))\\\n",
    "    .takeOrdered(10, key=lambda t: -t[1][0] / t[1][1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
