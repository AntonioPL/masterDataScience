{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Flata Data: Functions\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_new_ip():\n",
    "    \"\"\"Change IP using TOR\"\"\"\n",
    "    \n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate(password='ju4n4n4290')\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_current_ip():\n",
    "    \"\"\"get current ip\"\"\"\n",
    "    \n",
    "    local_proxy = 'socks5://localhost:9050'\n",
    "    socks_proxy = {\n",
    "        'http': local_proxy,\n",
    "        'https': local_proxy\n",
    "    }\n",
    "    \n",
    "    current_ip = requests.get(url='http://icanhazip.com/',\n",
    "                              proxies=socks_proxy,\n",
    "                              verify=False)\n",
    "    \n",
    "    return current_ip.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_change_ip():\n",
    "    \"\"\"test if the IP changes properly\"\"\"\n",
    "    \n",
    "    old_ip = get_current_ip()\n",
    "    set_new_ip()\n",
    "    new_ip = get_current_ip()\n",
    "    \n",
    "    if old_ip == new_ip:\n",
    "        return \"Error: IP has not changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"get soup requesting a url through Tor and Privoxy\"\"\"\n",
    "    \n",
    "    local_proxy = 'socks5://localhost:9050'\n",
    "    socks_proxy = {\n",
    "        'http': local_proxy,\n",
    "        'https': local_proxy\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url, proxies=socks_proxy, verify=False)\n",
    "\n",
    "    page = r.content\n",
    "    soup = BeautifulSoup(page, 'html5lib')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Flat ID and Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_of_properties_for_sale(city):\n",
    "    \"\"\"\n",
    "    Get the number of properties for sale at the moment of requesting. \n",
    "    This function has only been tested for the cities of Mostoles, Leganes, Fuenlabrada and Getafe\n",
    "    \"\"\"\n",
    "    \n",
    "    set_new_ip()\n",
    "    test_change_ip()\n",
    "    \n",
    "    url_search = \"https://www.idealista.com/venta-viviendas/\"+city+\"-madrid/con-pisos/\"\n",
    "    html_search = get_soup(url_search)\n",
    "    \n",
    "    string_properties = html_search.find_all(\"span\", class_=\"breadcrumb-info\")[2].get_text().replace(\".\",\"\")\n",
    "    \n",
    "    properties = int(string_properties)\n",
    "    print(\"There are {} properties in {}\".format(properties,city))\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_pages(number_of_properties):\n",
    "    \"\"\"Number of pages to scrap in the first search page\"\"\"\n",
    "    \n",
    "    pages = int(number_of_properties/30)+1 # there are 30 properties for page\n",
    "    print(\"There are {} pages in the first search page\".format(pages))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_links(number_of_pages,city):\n",
    "    \"\"\"Get the links of the search page for scraping\"\"\"\n",
    "    \n",
    "    links = []\n",
    "    for page in range(number_of_pages):\n",
    "        page += 1\n",
    "        \n",
    "        url = \"https://www.idealista.com/venta-viviendas/{}-madrid/con-pisos/pagina-{}.htm\".format(city,page)\n",
    "        links.append(url)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_properties(ids_properties,prices_properties):\n",
    "    \"\"\"\n",
    "    Save id and price properties in a dataframe.\n",
    "    Transform prices into integers and get the link of the flat from id\n",
    "    \"\"\"\n",
    "    properties = pd.DataFrame({\n",
    "        \"price\" : prices_properties,\n",
    "        \"id\" : ids_properties\n",
    "    })\n",
    "    properties['id'] = properties['id'].astype(int)\n",
    "    \n",
    "    properties['price'] = properties['price'].map(lambda x:int(x.replace(\"â‚¬\",\"\").replace(\".\",\"\")))\n",
    "    \n",
    "    properties['link'] = properties['id'].map(lambda x:\"https://www.idealista.com/inmueble/{}/\".format(x))\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id_and_price(search_links):\n",
    "    \"\"\"Get the id and price of each flat from search links\"\"\"\n",
    "    \n",
    "    ids_properties = []\n",
    "    prices_properties = []\n",
    "\n",
    "    set_new_ip()\n",
    "    test_change_ip()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for link in search_links:\n",
    "\n",
    "        counter += 1\n",
    "        print(\"{} / {}\".format(counter,len(search_links)))\n",
    "\n",
    "        flat_and_price = get_soup(link)\n",
    "\n",
    "        flat_ids = flat_and_price.find_all(\"a\", class_=\"item-link\")\n",
    "        flat_prices = flat_and_price.find_all(\"span\", class_=\"item-price h2-simulated\")\n",
    "        \n",
    "        for ids in flat_ids:\n",
    "            print(ids.get(\"href\").split(\"/\")[2])\n",
    "            id_flat = ids.get(\"href\").split(\"/\")[2]\n",
    "            \n",
    "            ids_properties.append(id_flat)\n",
    "\n",
    "        for prices in flat_prices:\n",
    "            print(prices.get_text())\n",
    "            price_flat = prices.get_text()\n",
    "            \n",
    "            prices_properties.append(price_flat)\n",
    "    \n",
    "    # process ids and prices\n",
    "    properties = process_properties(ids_properties, prices_properties)\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_old_and_new_properties(city):\n",
    "    \"\"\"Get the update properties and merge them to the old ones\"\"\"\n",
    "    \n",
    "    date = str(datetime.datetime.now())[:10]\n",
    "    path_old = \"./data/clean/{}/{}_properties.csv\".format(city, city)\n",
    "    path_new = \"./data/raw/{}/properties_{}.csv\".format(city, date)\n",
    "    \n",
    "    properties_old = pd.read_csv(path_old, sep = \"^\")\n",
    "    properties_new = pd.read_csv(path_new, sep = \"^\")\n",
    "    \n",
    "    print(\"Merging old and new properties...\")\n",
    "    properties_merged = properties_old.merge(properties_new, left_on=\"id\", right_on=\"id\", how=\"outer\")\n",
    "    \n",
    "    # drop columns with links\n",
    "    cols_with_links = properties_merged.columns.str.contains(\"link\")\n",
    "    properties_merged = properties_merged.ix[:,~cols_with_links]\n",
    "    \n",
    "    # update price\n",
    "    price_update = \"price_{}\".format(date)\n",
    "    properties_merged[price_update] = properties_merged['price']\n",
    "    properties_merged.drop(\"price\", axis = 1, inplace = True)\n",
    "    \n",
    "    return properties_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_properties(city):\n",
    "    \"\"\"\n",
    "    This function gets the properties (ID, price and link to idealista web page) of the input city\n",
    "    Write properties in raw data folder\n",
    "    Merge new properties to the old ones and write the merged dataframe in clean data folder \n",
    "    with the following structure:\n",
    "    \n",
    "    |  id  |  price_date1  |  price_dateX  |  ...  |\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # first search for getting the number of properties in the city at idealista \n",
    "    # and the number of pages we have to scrap\n",
    "    number_of_properties = get_number_of_properties_for_sale(city)\n",
    "    pages = number_of_pages(number_of_properties)\n",
    "    \n",
    "    # creating links we are going to scrap\n",
    "    links = get_search_links(pages, city)\n",
    "    \n",
    "    # scraping pages for getting id and price. Then, create the link of each flat\n",
    "    properties_id_and_price = get_id_and_price(links)\n",
    "    \n",
    "    # date of today for creating a new folder where we are going to save the dataframe\n",
    "    date = str(datetime.datetime.now())[:10]\n",
    "    \n",
    "    # writing raw data\n",
    "    path_raw = \"./data/raw/{}/properties_{}.csv\".format(city, date)\n",
    "    properties_id_and_price.to_csv(path_raw, sep = \"^\", index = False)\n",
    "    \n",
    "    # merging old and new properties in a dataframe\n",
    "    properties_update = merge_old_and_new_properties(city)\n",
    "    path_clean = \"./data/clean/{}/{}_properties.csv\".format(city, city)\n",
    "    properties_update.to_csv(path_clean, sep = \"^\", index = False)\n",
    "    \n",
    "    return properties_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Attributes Data from Flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_set_new_ip(n_times, link):\n",
    "    \"\"\"\n",
    "    This functions is specific for getting attributes at each property page (link)\n",
    "    It tries to get the price by url request and if it does not get any information change the IP\n",
    "    and repeat the process n times.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < n_times:\n",
    "        try:\n",
    "            soup = get_soup(link)\n",
    "            price = soup.find_all('span', class_='h3-simulated txt-bold')[0].get_text()\n",
    "            break\n",
    "        \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            print(\"Trying to set another IP\")\n",
    "            set_new_ip()\n",
    "            test_change_ip()\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_links(city):\n",
    "    \"\"\"\n",
    "    Get the links of new flats published in a city\n",
    "    IMPORTANT!!!: get_propeties(city) has to be run in order to get the new flats previously\n",
    "    \"\"\"\n",
    "    path = \"./data/clean/{}/{}_properties.csv\".format(city, city)\n",
    "    properties_new = pd.read_csv(path, sep = \"^\")\n",
    "    \n",
    "    ids_new = properties_new[np.isnan(properties_new.ix[:,-2])]['id']\n",
    "    \n",
    "    links_new = ids_new.map(lambda x:\"https://www.idealista.com/inmueble/{}/\".format(x))\n",
    "    \n",
    "    return links_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_attributes(property_links):\n",
    "    \"\"\"\n",
    "    This function goes over property links and gets some flat attributes.\n",
    "    The IP has to be changed each n requests for avoiding to be banned.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attributes to scrap\n",
    "    att_id = []\n",
    "    att_price = []\n",
    "    att_main = {}\n",
    "    att_build = {}\n",
    "    att_equipment = {}\n",
    "    att_location = {}\n",
    "    \n",
    "    set_new_ip()\n",
    "    test_change_ip()\n",
    "    \n",
    "    counter = 0\n",
    "    print(\"There are {} new properties\".format(len(property_links)))\n",
    "    \n",
    "    for link in property_links:\n",
    "        \n",
    "        print(link)\n",
    "        counter += 1\n",
    "        print(\"{} / {}\".format(counter,len(property_links)))\n",
    "\n",
    "        print(get_current_ip())\n",
    "        if counter % 100 == 0:\n",
    "            set_new_ip()\n",
    "\n",
    "        html_flat = get_soup(link)\n",
    "\n",
    "        try_set_new_ip(10, link)\n",
    "\n",
    "        # id\n",
    "        id_number = link.split(\"/\")[4]\n",
    "        att_id.append(id_number) # id\n",
    "        \n",
    "        print(id_number)\n",
    "\n",
    "        # price\n",
    "        try: \n",
    "            price = html_flat.find_all('span', class_='h3-simulated txt-bold')[0].get_text()\n",
    "    \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            price = [None]\n",
    "        \n",
    "        att_price.append(price)\n",
    "        print(price)\n",
    "\n",
    "        # attributes\n",
    "        flat_attributes = html_flat.find_all('div', class_='details-property_features')\n",
    "        \n",
    "        # main attributes\n",
    "        try:\n",
    "            number_of_main_attributes = len(flat_attributes[0].find_all(\"li\"))\n",
    "\n",
    "            ids_main = []\n",
    "            for main_attribute in range(number_of_main_attributes):\n",
    "                flat_main_attribute = flat_attributes[0].find_all(\"li\")[main_attribute].get_text()\n",
    "                ids_main.append(flat_main_attribute)\n",
    "                \n",
    "                att_main[id_number] = ids_main\n",
    "        \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            att_main[id_number] = [None]\n",
    "        \n",
    "        # build attributes\n",
    "        try:    \n",
    "            number_of_build_attributes = len(flat_attributes[1].find_all(\"li\"))\n",
    "\n",
    "            ids_build = []\n",
    "            for build_attribute in range(number_of_build_attributes):\n",
    "                flat_build_attribute = flat_attributes[1].find_all(\"li\")[build_attribute].get_text()\n",
    "                ids_build.append(flat_build_attribute)\n",
    "                \n",
    "                att_build[id_number] = ids_build\n",
    "        \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            att_build[id_number] = [None]\n",
    "        \n",
    "        # equipment attributes\n",
    "        try:\n",
    "            number_of_equipment_attributes = len(flat_attributes[2].find_all(\"li\"))\n",
    "\n",
    "            ids_equipment = []\n",
    "            for equipment_attribute in range(number_of_equipment_attributes):\n",
    "                flat_equipment_attribute = flat_attributes[2].find_all(\"li\")[equipment_attribute].get_text()\n",
    "                ids_equipment.append(flat_equipment_attribute)\n",
    "                \n",
    "                att_equipment[id_number] = ids_equipment\n",
    "                \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            att_equipment[id_number] = [None]\n",
    "        \n",
    "        # location\n",
    "        try:\n",
    "            location = html_flat.find_all('div', class_='ide-box-detail overlay-box')[2].find_all(\"li\")\n",
    "            number_of_location_attributes = len(location)\n",
    "            \n",
    "            ids_location = []\n",
    "            for location_attribute in range(number_of_location_attributes):\n",
    "                flat_location_attribute = location[location_attribute].get_text()\n",
    "                ids_location.append(flat_location_attribute)\n",
    "                \n",
    "                att_location[id_number] = ids_location\n",
    "        \n",
    "        except:\n",
    "            \"list index out of range\"\n",
    "            att_location[id_number] = [None]\n",
    "        \n",
    "    # processing attributes\n",
    "    att_main_to_dict = pd.DataFrame.from_dict(att_main,orient='index')\n",
    "    att_build_to_dict = pd.DataFrame.from_dict(att_build,orient='index')\n",
    "    att_equipment_to_dict = pd.DataFrame.from_dict(att_equipment,orient='index')\n",
    "    att_location_to_dict = pd.DataFrame.from_dict(att_location,orient='index')\n",
    "    \n",
    "    attributes = pd.concat([att_main_to_dict, att_build_to_dict, att_equipment_to_dict, att_location_to_dict],\n",
    "                                axis = 1)\n",
    "    \n",
    "    attributes['price'] = att_price\n",
    "\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns_names(attributes_columns):\n",
    "    \"\"\"parsing column names in the form att_main_X, att_build_X, att_equipment_X, att_location_X and price\"\"\"\n",
    "    \n",
    "    \n",
    "    names = [\"att_main\",\"att_build\",\"att_equipment\",\"att_location\"]\n",
    "    \n",
    "    new_columns = []\n",
    "    \n",
    "    counter_name = 0\n",
    "    counter_column = 0\n",
    "\n",
    "    for column_name in attributes_columns:\n",
    "        try:\n",
    "            new_name = names[counter_name]+\"_\"+str(column_name)\n",
    "            new_columns.append(new_name)\n",
    "\n",
    "            if attributes_columns[counter_column] >= attributes_columns[counter_column+1]:\n",
    "                counter_name += 1\n",
    "            counter_column += 1\n",
    "        except:\n",
    "            \"'>=' not supported between instances of 'int' and 'str'\"\n",
    "    \n",
    "    new_columns[-1] = \"price\"\n",
    "    \n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_attributes(attributes):\n",
    "    \"\"\"process attributes dataframe so that all the attributes dataframes have the same structure\"\"\"\n",
    "    \n",
    "    structure = ['att_main_0','att_main_1', 'att_main_2', 'att_main_3', 'att_main_4',\n",
    "                 'att_main_5', 'att_main_6', 'att_main_7', 'att_main_8', 'att_main_9',\n",
    "                 'att_main_10', 'att_main_11', 'att_main_12', 'att_main_13', 'att_main_14', 'att_main_15',\n",
    "                 'att_build_0','att_build_1', 'att_build_2', 'att_build_3','att_equipment_0', 'att_equipment_1',\n",
    "                 'att_equipment_2', 'att_equipment_3', 'att_equipment_4','att_location_0', 'att_location_1',\n",
    "                 'att_location_2', 'att_location_3', 'att_location_4', 'att_location_5', 'att_location_6',\n",
    "                 'att_location_7', 'att_location_8', 'price']\n",
    "    \n",
    "    cols = attributes.columns\n",
    "    \n",
    "    for column_name in structure:\n",
    "        if column_name not in attributes.columns:\n",
    "            attributes[column_name] = np.nan\n",
    "    \n",
    "    return attributes[structure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_new_attributes(city):\n",
    "    \"\"\"update flats and concat new flats to the old ones\"\"\"\n",
    "    \n",
    "    # get new flats\n",
    "    attributes_new = get_attributes(get_new_links(city))\n",
    "    \n",
    "    # parse column names\n",
    "    attributes_new.columns = parse_columns_names(attributes_new.columns)\n",
    "    \n",
    "    # process attributes in the proper structure\n",
    "    print(\"New flats processing...\")\n",
    "    attributes_new = process_attributes(attributes_new)\n",
    "    print(attributes_new.head())\n",
    "    \n",
    "    # Writing new flats in the raw folder\n",
    "    date = str(datetime.datetime.now())[:10]\n",
    "    attributes_new.to_csv(\"data/raw/{}/attributes_{}.csv\".format(city,date))\n",
    "    \n",
    "    # read old flats\n",
    "    print(\"Reading old flats\")\n",
    "    attributes_old = pd.read_csv(\"data/clean/{}/{}_attributes.csv\".format(city, city), sep = \"^\").set_index(\"Unnamed: 0\")\n",
    "    print(attributes_old.head())\n",
    "    \n",
    "    # concat new flats to the old ones\n",
    "    attributes = pd.concat([attributes_old, attributes_new], axis = 0)\n",
    "    print(attributes_new.head())\n",
    "    \n",
    "    # write attributes\n",
    "    print(\"Updating new flats\")\n",
    "    attributes.to_csv(\"data/clean/{}/{}_attributes.csv\".format(city, city), sep = \"^\")\n",
    "    print(\"Flats updated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
